# APP CONFIG
server.port=8080
server.servlet.context-path=/backend
# Elasticsearch
elasticsearch.host=35.228.12.206
elasticsearch.port=9200
elasticsearch.schema=http
elasticsearch.index=index
elasticsearch.user=user
elasticsearch.password=pwd
# Inteval between running scheduled task for syncing to Elasticsearch
esindexingjob.interval.seconds=60
# Database
spring.datasource.url=${POSTGRES_URL:jdbc:postgresql://localhost:5432/postgres}
spring.datasource.hikari.minimum-idle=1
spring.datasource.hikari.maximum-pool-size=2
spring.jpa.properties.hibernate.jdbc.lob.non_contextual_creation=true
spring.jpa.hibernate.ddl-auto=none
# Disable feature detection by this undocumented parameter. Check the org.hibernate.engine.jdbc.internal.JdbcServiceImpl.configure method for more details.
spring.jpa.properties.hibernate.temp.use_jdbc_metadata_defaults=false
# Because detection is disabled you have to set correct dialect by hand.
spring.jpa.database-platform=org.hibernate.dialect.PostgreSQL9Dialect
# Flyway
spring.flyway.schemas=public
spring.flyway.table=flyway_backend_history
spring.flyway.baseline-on-migrate=true
# Endpoints
datacatalog.policy.url=${POLICY_URL:http://localhost:8080/policies/policy}
nais.elector.path=${ELECTOR_PATH:localhost:8080/elector}

management.endpoints.web.base-path=/internal
management.endpoints.web.exposure.include=info,health,prometheus
management.endpoints.web.path-mapping.prometheus=metrics
management.endpoint.health.show-details=always
management.endpoint.security.enabled=false
management.endpoint.metrics.enabled=false
management.endpoint.prometheus.enabled=true
management.info.git.mode=simple
# Github
github.host=${GITHUB_HOST:api.github.com}
github.port=${GITHUB_PORT:-1}
github.scheme=${GITHUB_SCHEME:https}
github.webhooks-secret=${GITHUB_WEBHOOKS_SECRET:5a0924fa6403443494b24fda6a0df269}
github.key-path=/etc/secret/datajegerne
github.app-id=26100
pol.datasett.owner=navikt
pol.datasett.repo=pol-datasett
# Kafka
backend.fss.user=${SRV_DATA_CATALOG_POLICIES_FSS_USER:kafkauser}
backend.fss.pwd=${SRV_DATA_CATALOG_POLICIES_FSS_PWD:kafkapwd}
kafka.topics.dataset=aapen-data-catalog-dataset-v1
spring.kafka.bootstrap-servers=${KAFKA_BOOTSTRAP_SERVERS:localhost:9092}
spring.kafka.consumer.key-deserializer=org.apache.kafka.common.serialization.StringDeserializer
spring.kafka.consumer.value-deserializer=io.confluent.kafka.serializers.KafkaAvroDeserializer
spring.kafka.consumer.auto-offset-reset=earliest
spring.kafka.consumer.enable-auto-commit=false
spring.kafka.consumer.max-poll-records=1
spring.kafka.consumer.group-id=data-catalog
spring.kafka.listener.ack-mode=manual
spring.kafka.listener.poll-timeout=1000
spring.kafka.properties.specific.avro.reader=true
spring.kafka.properties.schema.registry.url=${KAFKA_SCHEMA_REGISTRY_URL:http://localhost:8082}
spring.kafka.properties.security.protocol=SASL_SSL
spring.kafka.properties.sasl.mechanism=PLAIN
spring.kafka.properties.sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username="${backend.fss.user}" password="${backend.fss.pwd}";
spring.kafka.properties.ssl.truststore.location=${javax.net.ssl.trustStore}
spring.kafka.properties.ssl.truststore.password=${javax.net.ssl.trustStorePassword}
#spring.kafka.properties.client-id=data-catalog-backend

kafka.rest.schema-registry-url=${KAFKA_REST_SCHEMA_REGISTRY_URL:http://localhost:8085}
kafka.rest.admin-url=${KAFKA_REST_ADMIN_URL:http://localhost:8086}
kafka.rest.admin-apikey=${KAFKA_REST_ADMIN_APIKEY:secret}
kafkaindexingjob.interval.seconds=3600
# Azure
azure.activedirectory.client-id=${AZURE_CLIENT_ID:client-id}
azure.activedirectory.client-secret=${AZURE_CLIENT_SECRET:client-secret}
azure.activedirectory.active-directory-groups=${AZURE_CLIENT_GROUPS:teamdatajegerne}
# Swagger
springfox.documentation.swagger.v2.path=/swagger-docs
# Logg
logging.level.org.springframework.vault=WARNING
logging.level.no.nav.data.catalog.backend.app.policy=WARNING
logging.level.no.nav.data.catalog.backend=WARNING